# -*- coding: utf-8 -*-
"""Anomaly Detection_Enron

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11SEStUCe0bxUfnVDTgMfzqBCxAZ32pct
"""

## import settings

import networkx as nx
import numpy as np
from layers import *
from config import *
from inits import *
from metrics import *
from models import *
from utils import *

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.metrics import *
# from config import args
import numpy as np


## data preprocessing

G = nx.read_graphml('./Enron/Enron.graphml') # Enron data with node and edge feature

att1 = ['EnronMailsTo','OtherMailsTo','AverageNumberTo','EnronMailsCc','OtherMailsCc','AverageNumberCc','EnronMailsBcc','OtherMailsBcc','AverageNumberBcc'] # Recipient and Sender Information
att2 = ['MimeVersionsCount','DifferentCosCount','DifferentCharsetsCount','DifferentEncodingsCount','AverageContentLength','AverageDifferentSymbolsContent','AverageContentForwardingCount','AverageContentReplyCount'] # Content Information
att3 = ['AverageSubjectLength','AverageDifferentSymbolsSubject','AverageRangeBetween2Mails'] # Subject and Frequency Information
all_att = att1+att2+att3 # node feature index each attribute is classified as different view

nd = list(G.nodes())
eg = list(G.edges())

node_0 = sorted(nd) # sequence of the node index
node = [] # n*20 - with node feature
edge = [] # n*n

for i in range(len(node_0)):
  n = []
  for j in range(20):
    n.append(float(0))
  node.append(n)

for i in range(len(node_0)):
  n = []
  for j in range(len(node_0)):
    n.append(0)
  edge.append(n)

t = 0
for att in all_att:
  temp = list(G.nodes.data(att))
  for n in temp:
    if n[1] is not None:
      node[node_0.index(n[0])][t] = float(n[1])
  t += 1

for n in eg: # the order of the node is same with node_0
  edge[node_0.index(n[0])][node_0.index(n[1])] = 1
  edge[node_0.index(n[1])][node_0.index(n[0])] = 1

X = np.array(node) # Node feature matrix (n * 20)
A = np.array(edge) # Adjacency matrix (n * n)

# GNN for different view
X1 = X[:,0:9] # Node feature matrix for view1
X2 = X[:,9:17] # Node feature matrix for view2
X3 = X[:,17:20] # Node feature matrix for view3


## model definition

# Generate GNN model structure
class GCN(keras.Model):
    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):
        super(GCN, self).__init__(**kwargs)

        self.input_dim = input_dim
        self.output_dim = output_dim

        print('input dim:', input_dim)
        print('output dim:', output_dim)
        print('num_features_nonzero:', num_features_nonzero)

        self.input1 = tf.keras.layers.InputLayer(input_shape=(self.input_dim[0], ))

        self.layer1 = GraphConvolution(input_dim=self.input_dim[0],
                                            output_dim=args.hidden1, 
                                            num_features_nonzero=num_features_nonzero[0],
                                            activation=tf.nn.relu,
                                            dropout=args.dropout,
                                            is_sparse_inputs=True)

        self.input2 = tf.keras.layers.InputLayer(input_shape=(self.input_dim[1],))

        self.layer2 = GraphConvolution(input_dim=self.input_dim[1],
                                            output_dim=args.hidden1, 
                                            num_features_nonzero=num_features_nonzero[1],
                                            activation=tf.nn.relu,
                                            dropout=args.dropout,
                                            is_sparse_inputs=True)

        self.input3 = tf.keras.layers.InputLayer(input_shape=(self.input_dim[2],))

        self.layer3 = GraphConvolution(input_dim=self.input_dim[2],
                                            output_dim=args.hidden1, 
                                            num_features_nonzero=num_features_nonzero[2],
                                            activation=tf.nn.relu,
                                            dropout=args.dropout,
                                            is_sparse_inputs=True)

        self.concate = tf.keras.layers.Concatenate(axis=1)
        self.dense = tf.keras.layers.Dense(20)
        # self.adjacency = tf.keras.layers.dot(axes=(2, 1))

        for p in self.trainable_variables:
            print(p.name, p.shape)

    def call(self, inputs, training=None):
        """
        :param inputs:
        :param training:
        :return:
        """
        x1, x2, x3 = inputs

        x_1 = self.input1(x1)
        x_2 = self.input1(x2)
        x_3 = self.input1(x3)

        out1 = self.layer1(x_1)
        out2 = self.layer2(x_2)
        out3 = self.layer3(x_3)
        
        z = self.concate(out1, out2, out3)
        self.x_tilde = self.dense(z)

        adj = tf.keras.layers.dot(axes=(2, 1))([z, tf.transpose(z)])
        self.a_tilde = tf.keras.activations.sigmoid(adj)

        return self.x_tilde, self.a_tilde

## loss function definition


def loss(y_true, y_pred):
    x, a = y_true
    x_tilde, a_tilde = y_pred

    loss_s = tf.keras.losses.categorical_crossentropy(a, a_tilde)
    loss_a = (np.linalg.norm(x-x_tilde))^2
    loss = loss_s + loss_a

    return loss

## model test


model = GCN([9, 8, 3], None, [9, 8, 3])
model.compile(optimizer='adam', loss=loss)
model.summary()


##

